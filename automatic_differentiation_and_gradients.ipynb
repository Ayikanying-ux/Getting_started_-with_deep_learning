{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwX2wrRXKMd3xRvXfFO2iJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ayikanying-ux/Getting_started_-with_deep_learning/blob/main/automatic_differentiation_and_gradients.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEDaNBAV0lxw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Automatic Differentiation and Gradients\n",
        "Automatic differentiation is useful for implementing machine learning algorithms such as backpropagation for training neural networks"
      ],
      "metadata": {
        "id": "atjTVazQ0t6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "K-OQT_CG1B3q"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Computing gradients\n",
        "To differentiate automatically, TensorFlow needs to remember what operations happen in what order during the forward pass, Then, during the backward pass, TensorFlow traverses the list of operations in reserve order to compute gradients"
      ],
      "metadata": {
        "id": "Ebv3sC0C1KY-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient tapes\n",
        "TensorFlow provides the tf.GradientTape API for automatic differentiation; that is, computing the gradient of a computation with respect to some inputs, usually tf.Variables. TensorFlow \"records\" relevant operations executed inside the context of a tf.GradientTape onto a \"tape\". TensorFlow then used that tape to compute the gradients of a \"recorded\" computation using reverse mode differentiation."
      ],
      "metadata": {
        "id": "urtzznEm17r1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.Variable(3.0)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  y = x**2"
      ],
      "metadata": {
        "id": "cU5-8DKF1JjN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "Once you've recorded some operations, use GradientTape.gradient(target, sources) to calculate teh gradient of some target (often a loss) relative to some source (often the model's varibles)"
      ],
      "metadata": {
        "id": "su33R2O13iSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dy_dx = tape.gradient(y, x)\n",
        "dy_dx.numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrtlbO313EQB",
        "outputId": "d5117cd6-0800-4162-a9f5-f58f1a2932bf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.0"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6zWT2jcZ3QhR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}