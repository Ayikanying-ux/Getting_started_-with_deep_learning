{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMyFK+EIp7o+tdDCl90GsFh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ayikanying-ux/Getting_started_-with_deep_learning/blob/main/automatic_differentiation_and_gradients.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IEDaNBAV0lxw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Automatic Differentiation and Gradients\n",
        "Automatic differentiation is useful for implementing machine learning algorithms such as backpropagation for training neural networks"
      ],
      "metadata": {
        "id": "atjTVazQ0t6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "K-OQT_CG1B3q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Computing gradients\n",
        "To differentiate automatically, TensorFlow needs to remember what operations happen in what order during the forward pass, Then, during the backward pass, TensorFlow traverses the list of operations in reserve order to compute gradients"
      ],
      "metadata": {
        "id": "Ebv3sC0C1KY-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient tapes\n",
        "TensorFlow provides the tf.GradientTape API for automatic differentiation; that is, computing the gradient of a computation with respect to some inputs, usually tf.Variables. TensorFlow \"records\" relevant operations executed inside the context of a tf.GradientTape onto a \"tape\". TensorFlow then used that tape to compute the gradients of a \"recorded\" computation using reverse mode differentiation."
      ],
      "metadata": {
        "id": "urtzznEm17r1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.Variable(3.0)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  y = x**2"
      ],
      "metadata": {
        "id": "cU5-8DKF1JjN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "Once you've recorded some operations, use GradientTape.gradient(target, sources) to calculate teh gradient of some target (often a loss) relative to some source (often the model's varibles)"
      ],
      "metadata": {
        "id": "su33R2O13iSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dy_dx = tape.gradient(y, x)\n",
        "dy_dx.numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrtlbO313EQB",
        "outputId": "de7734c2-87b8-47bf-8afd-e205405269f6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.0"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "The above example uses scalars, but tf.GradientTape works as easily on any tensor:\n"
      ],
      "metadata": {
        "id": "fGi-HrCv5zrC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w = tf.Variable(tf.random.normal((3, 2)), name=\"w\")\n",
        "b = tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\n",
        "x = [[1., 2., 3.]]\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  y = x @ w + b\n",
        "  loss = tf.reduce_mean(y**2)"
      ],
      "metadata": {
        "id": "6zWT2jcZ3QhR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "To get the gradient of loss with respect to both variables, you can pass both as sources to the gradient method. The tape is flexible about how sources are passed and will accept any nested combination of lists or dictionaries and return the gradient structured the same way (see tf.nest).\n"
      ],
      "metadata": {
        "id": "kEaqj2XrNW0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[dl_dw, dl_db] = tape.gradient(loss, [w, b])"
      ],
      "metadata": {
        "id": "GvGDWVjq6q3_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(w.shape)\n",
        "print(dl_dw.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Vji95l_NqkC",
        "outputId": "bf077ec7-59a9-4abe-a596-3ee6d86eaae3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 2)\n",
            "(3, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_vars={\n",
        "    'w': w,\n",
        "    'b': b\n",
        "}\n",
        "grad = tape.gradient(loss, my_vars)\n",
        "grad['b']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoQWNwD1Nv_0",
        "outputId": "2eed6b8d-3266-4673-f670-51e2442c5ef3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([3.93863  , 1.7535148], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient with respect to a model\n",
        "- It's common to collect ```tf.Variables``` into ```tf.Module``` or one of its subclassed (```layers.Layer```, ```keras.Model```) fo checking and exporting.\n",
        "\n",
        "- In most cases, you will want to calculate gradients with respect to a model's trainable variables. Since all subclasses of ```tf.Module``` aggregate their variables in the ```Module.trainable_variables``` property, you can calculate these gradients in a few lines of code"
      ],
      "metadata": {
        "id": "3zojz7xxOMfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer = tf.keras.layers.Dense(2, activation='relu')\n",
        "x = tf.constant([[1., 2., 3.]])\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  # Forward pass\n",
        "  y = layer(x)\n",
        "  loss = tf.reduce_mean(y**2)\n",
        "\n",
        "# Calculate gradients with respect to every trainable variable\n",
        "grad = tape.gradient(loss, layer.trainable_variables)"
      ],
      "metadata": {
        "id": "WEmdIz6wN-7M"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for var, g in zip(layer.trainable_variables, grad):\n",
        "  print(f'{var.name}, shape: {g.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-LKVFwwPzQ8",
        "outputId": "6c9a95be-6b19-4986-9763-ea11bb2eba2f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dense_1/kernel:0, shape: (3, 2)\n",
            "dense_1/bias:0, shape: (2,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Controlling what the tape watches\n",
        "The default behaviour is to record all operations after accessing a trainable ```tf.Variable```.\n",
        "\n",
        "The reasons for this are:\n",
        "- The tape needs to know which operations to record in the forward pass to calculate the gradients in the backwards pass.\n",
        "- The tape holds references to intermediate outputs, so you don't want to record unnecessary operations.\n",
        "- The most common use case involves calculating the gradient of a loss with respect to all a model's trainable variables.\n"
      ],
      "metadata": {
        "id": "bVsGkWHpQYQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A trainable variable\n",
        "x0 = tf.Variable(3.0, name='x0')\n",
        "\n",
        "# Not trainable\n",
        "x1 = tf.Variable(3.0, name=\"x1\", trainable=False)\n",
        "\n",
        "# Not a Variable: A variable + tensor returns a tensor\n",
        "x2 = tf.Variable(2.0, name='x2')+1.0\n",
        "\n",
        "# Not a Variable\n",
        "x3 = tf.constant(3.0, name='x3')\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  y=(x0**2)+(x1**2)+(x2**2)\n",
        "\n",
        "grad = tape.gradient(y, [x0, x1, x2, x3])\n",
        "\n",
        "for g in grad:\n",
        "  print(g)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdru-vmVQNeg",
        "outputId": "0eb0c94d-1d14-42b1-d9b5-25870385ce48"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(6.0, shape=(), dtype=float32)\n",
            "None\n",
            "None\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "You can list the variables being watched by the tape using the GradientTape.watched_variables method:\n"
      ],
      "metadata": {
        "id": "KRP3KEaQSbjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[var.name for var in tape.watched_variables()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dL5WuLKiSDKr",
        "outputId": "44efacb0-8382-4956-91c9-cd82b8735688"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['x0:0']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "* ```tf.GradientTape``` provides hooks that give the user control over what is or is not watched.\n",
        "* To record gradients with respect to a ```tf.Tensor```, you need to call ```GradientTape.watch(x)```:"
      ],
      "metadata": {
        "id": "1IDr6E0tcDpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x=tf.constant(3.0)\n",
        "with tf.GradientTape() as tape:\n",
        "  tape.watch(x)\n",
        "  y = x**2\n",
        "\n",
        "dy_dx = tape.gradient(y, x)\n",
        "print(dy_dx.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvW8mreaSi66",
        "outputId": "d1618280-50e4-45a3-eb4c-eacbed0164ad"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "Conversely, to disable the default behavior of watching all ```tf.Variables```, set ```watch_accessed_variables=False``` when creating the gradient tape. This calculation uses two variables, but only connects the gradient for one of the variables:"
      ],
      "metadata": {
        "id": "ABOI_Sq5dAyv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yE7JxFiWcpla"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}